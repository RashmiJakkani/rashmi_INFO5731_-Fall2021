{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "In-class-exercise-02.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rjakkani1015/rashmi_INFO5731_-Fall2021/blob/main/In_class_exercise_02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLQVz5HMQAd8"
      },
      "source": [
        "## The third In-class-exercise (9/15/2021, 40 points in total)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twLNd6WXQAd9"
      },
      "source": [
        "The purpose of this exercise is to understand users' information needs, then collect data from different sources for analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tD0lmxKtQAd-"
      },
      "source": [
        "Question 1 (10 points): Describe an interesting research question (or practical question) you have in mind, what kind of data should be collected to answer the question(s)? How many data needed for the analysis? The detail steps for collecting and save the data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYpHLSB8QAd-"
      },
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        "The amount of data generated is increasing on a daily basis. According to Forbes, 90% of the data available today was generated in the last 2 years, this explains us to what extent data has become a part of our daily life. Major reason for this is the increase in the number of data touch points, due to the mass production of electronics, especially affordable smart devices.\n",
        "\n",
        "With the advancement of internet of things, an ecosystem of devices can be created that can track the movements of individuals who stay alone. For example, the data being generated from the smart devices like phones and watches can be integrated with the local emergency services, who can get alerted when there is some deviation in the pulse or heartbeat of the person using it. By this, the emergency services can be alerted without any manual intervention.\n",
        "\n",
        "Another way in which this data can be used is to identify the reactions of employees in an organization. This helps them in analyzing the satisfaction level of their employees and come up with programs to improve the employee satisfaction which in turn can control the attrition rates.\n",
        "\n",
        "Choosing the right target segment is very important in sample collection, for example if we want to collect data to alert the emergency services about heart attacks, the data should be collected from elderly and not from youth.\n",
        "\n",
        "More the data, more accurate the predictions will be. So, itâ€™s important to collect as much data as possible, like more than 10000 records would give the right predictions. The model should be intelligent enough to learn from the new data being generated and become smarter day by day.\n",
        "\n",
        "The steps for collecting the data can be done from any available data repositories in the market. After collecting the data cleanup is very important to make sure there is no junk data that might lead to wrong predictions. Once the data is cleaned up based upon our requirements the right prediction model should be chosen for analysis.\n",
        "\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QwhsewSyQAd_"
      },
      "source": [
        "Question 2 (10 points): Write python code to collect 1000 reviews of a movie from IMDB (https://www.imdb.com/) or 1000 reviews of a product from Amazon (https://www.amazon.com/).\n",
        "\n",
        "As for the IMDB movie review, the following informtion need to be collected (for example: https://www.imdb.com/title/tt6751668/reviews?ref_=tt_urv):\n",
        "\n",
        "(1) User name\n",
        "\n",
        "(2) Star\n",
        "\n",
        "(3) Review title\n",
        "\n",
        "(4) Review text\n",
        "\n",
        "(5) Review posted time\n",
        "\n",
        "\n",
        "As for the Amazon product review, the following information need to be collected (for example: https://www.amazon.com/Hands-Machine-Learning-Scikit-Learn-TensorFlow/dp/1492032646/ref=sr_1_3?crid=2E3C55VKJX0K3&dchild=1&keywords=machine+learning+andrew+ng&qid=1631718619&sr=8-3):\n",
        "\n",
        "(1) User name\n",
        "\n",
        "(2) Star\n",
        "\n",
        "(3) Review title\n",
        "\n",
        "(4) Review text\n",
        "\n",
        "(5) Review posted time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E6gtegUbQAeA",
        "outputId": "c00fec44-3e71-4ca1-bf00-583e1a14ffd2"
      },
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "d = {}\n",
        "df = pd.DataFrame()\n",
        "\n",
        "# url = 'https://www.imdb.com/title/tt6751668/reviews?ref_=tt_urv'\n",
        "data_ajaxurl = \"/title/tt6751668/reviews/_ajax\"\n",
        "data_key = \"g4wp7cber43te3qa7kthxnzwr7r44bjhzfmxvlnomwklyczuf43o6ss6ouyf7nzpdb4k5f3emasopxdsnumnyaizt7doony\"\n",
        "url = \"http://www.imdb.com/\" + data_ajaxurl + \"?paginationKey=\" + data_key\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.text, 'lxml')\n",
        "\n",
        "counter = 0\n",
        "\n",
        "for i in soup.findAll(\"div\", {'class': \"review-container\"}):\n",
        "\n",
        "    print(counter)\n",
        "\n",
        "    username = i.find(\"span\", {'class': \"display-name-link\"}).text\n",
        "\n",
        "    star_info = i.find(\"span\", {'class': \"rating-other-user-rating\"})\n",
        "    star = str(10) if star_info is None else star_info.text\n",
        "\n",
        "    review_title = i.find(\"a\", {'class': \"title\"}).text\n",
        "    review_text = i.find(\"div\", {'class': \"text show-more__control\"}).text\n",
        "    review_posted_time = i.find(\"span\", {'class': \"review-date\"}).text\n",
        "\n",
        "    d = {\n",
        "        'username': username,\n",
        "        'star': star,\n",
        "        'review_title': review_title,\n",
        "        'review_text': review_text,\n",
        "        'review_posted_time': review_posted_time\n",
        "    }\n",
        "\n",
        "    counter = counter + 1\n",
        "\n",
        "    if counter > 1000:\n",
        "        break\n",
        "\n",
        "    df = df.append(d, ignore_index=True)\n",
        "\n",
        "print(df.head())\n",
        "print(len(df))\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "  review_posted_time  ...     username\n",
            "0   10 February 2020  ...   federovsky\n",
            "1     7 January 2020  ...  genosypheus\n",
            "2   24 February 2020  ...   Prismark10\n",
            "3   31 December 2019  ...    Mustang92\n",
            "4      15 March 2020  ...    dvdboss-2\n",
            "\n",
            "[5 rows x 5 columns]\n",
            "25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8qsGg3bQAeB"
      },
      "source": [
        "Question 3 (10 points): Write python code to collect 1000 articles from Google Scholar (https://scholar.google.com/), Microsoft Academic (https://academic.microsoft.com/home), or CiteSeerX (https://citeseerx.ist.psu.edu/index), or Semantic Scholar (https://www.semanticscholar.org/). \n",
        "\n",
        "The following information of the article needs to be collected:\n",
        "\n",
        "(1) Title\n",
        "\n",
        "(2) Venue/journal/conference being published\n",
        "\n",
        "(3) Year\n",
        "\n",
        "(4) Authors\n",
        "\n",
        "(5) Abstract"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gqSePn8KQAeB",
        "outputId": "df496d60-a4c6-4cec-96ca-fd27a93d8348"
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests, lxml, os, json\n",
        "from dateutil.parser import parse\n",
        "import pandas as pd\n",
        "\n",
        "proxies = {\n",
        "    'http': os.getenv('HTTP_PROXY')  # or just type proxy here without os.getenv()\n",
        "}\n",
        "\n",
        "headers = {\n",
        "    'User-agent':\n",
        "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.102 Safari/537.36 Edge/18.19582\"\n",
        "}\n",
        "\n",
        "params = {\n",
        "    \"q\": \"machine\"\n",
        "}\n",
        "\n",
        "df = pd.DataFrame()\n",
        "\n",
        "for i in range(0, 100):\n",
        "    start = str(i * 10)\n",
        "\n",
        "    html = requests.get('https://scholar.google.com/scholar?start=' + start + '&q=machine&hl=en&as_sdt=0,22',\n",
        "                        headers=headers, proxies=proxies).text\n",
        "    soup = BeautifulSoup(html, 'lxml')\n",
        "\n",
        "    for result in soup.select('.gs_ri'):\n",
        "        data = {}\n",
        "\n",
        "        title_section = result.select('h3 > a')\n",
        "        if len(title_section) > 0:\n",
        "            section = title_section[0]\n",
        "            title = section.get_text()\n",
        "\n",
        "        authors_section = result.select('.gs_a')\n",
        "        if len(authors_section) > 0:\n",
        "            section = authors_section[0]\n",
        "            section_text = section.get_text()\n",
        "            section_split = section_text.split('-')\n",
        "            authors = section_split[0]\n",
        "            venue = section_split[1]\n",
        "            year = venue[-4:]\n",
        "\n",
        "        abstract_section = result.select('.gs_rs')\n",
        "        if len(abstract_section) > 0:\n",
        "            section = abstract_section[0]\n",
        "            abstract = section.get_text()\n",
        "\n",
        "        d = {\n",
        "            'title': title,\n",
        "            'venue': venue,\n",
        "            'year': year,\n",
        "            'authors': authors,\n",
        "            'abstract': abstract\n",
        "        }\n",
        "\n",
        "        df = df.append(d, ignore_index=True)\n",
        "\n",
        "print(len(df))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "600\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2KjJHvwKQAeC"
      },
      "source": [
        "Question 4 (10 points): Write python code to collect 1000 posts from Twitter, or Facebook, or Instagram. You can either use hashtags, keywords, user_name, user_id, or other information to collect the data. \n",
        "\n",
        "The following information needs to be collected:\n",
        "\n",
        "(1) User_name\n",
        "\n",
        "(2) Posted time\n",
        "\n",
        "(3) Text "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XeT8yQ3uQAeC",
        "outputId": "a580aba9-87d8-4eee-a2fd-00fc1aa5ae12"
      },
      "source": [
        "import tweepy\n",
        "import csv\n",
        "import pandas as pd\n",
        "\n",
        "access_token = '781950288466935808-qJzrIDv0gv9giCmMZATqxII52juoHIW'\n",
        "access_token_secret = 'gX35Ak4P8JB4s5xqEuMZNwOwO5L8bAbZhaXHHPnWQMaI0'\n",
        "consumer_key = 'gbsa3hkFg4nEn5FvKHxRAmRZq'\n",
        "consumer_secret = 'mygRjbiLGd40Q1HGbfObSpUCAZEGZVdDmpD8mPLJBAs4Kq6qAR'\n",
        "\n",
        "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
        "auth.set_access_token(access_token, access_token_secret)\n",
        "api = tweepy.API(auth, wait_on_rate_limit=True)\n",
        "\n",
        "df = pd.DataFrame()\n",
        "\n",
        "for tweet in tweepy.Cursor(api.search, q=\"#samantha\", count=10,\n",
        "                           lang=\"en\",\n",
        "                           since=\"2017-04-03\").items():\n",
        "    d = {}\n",
        "    username = tweet.user.name\n",
        "    posted_time = tweet.created_at\n",
        "    text = tweet.text\n",
        "\n",
        "    d = {\n",
        "        'username': username,\n",
        "        'posted_time': posted_time,\n",
        "        'text': text\n",
        "    }\n",
        "\n",
        "    df = df.append(d, ignore_index=True)\n",
        "\n",
        "print(df.head())\n",
        "\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          posted_time  ...                  username\n",
            "0 2021-09-19 18:39:30  ...            Gurunadh Reddy\n",
            "1 2021-09-19 18:38:52  ...  Goddess Samantha Devotee\n",
            "2 2021-09-19 18:37:59  ...  Goddess Samantha Devotee\n",
            "3 2021-09-19 18:36:55  ...  Goddess Samantha Devotee\n",
            "4 2021-09-19 18:36:26  ...                     Sneha\n",
            "\n",
            "[5 rows x 3 columns]\n"
          ]
        }
      ]
    }
  ]
}