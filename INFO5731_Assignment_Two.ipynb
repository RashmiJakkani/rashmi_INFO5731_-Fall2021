{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "INFO5731_Assignment_Two.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rjakkani1015/rashmi_INFO5731_-Fall2021/blob/main/INFO5731_Assignment_Two.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USSdXHuqnwv9"
      },
      "source": [
        "# **INFO5731 Assignment Two**\n",
        "\n",
        "In this assignment, you will try to gather text data from open data source via web scraping or API. After that you need to clean the text data and syntactic analysis of the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWxodXh5n4xF"
      },
      "source": [
        "# **Question 1**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TenBkDJ5n95k"
      },
      "source": [
        "(20 points). Write a python program to collect text data from **either of the following sources** and save the data into a **csv file**:\n",
        "\n",
        "(1) Collect all the customer reviews of the product [Apple iPhone 11](https://www.amazon.com/Apple-iPhone-11-64GB-Unlocked/dp/B07ZPKF8RG/ref=sr_1_13?dchild=1&keywords=iphone+12&qid=1631721363&sr=8-13) on amazon.\n",
        "\n",
        "(2) Collect all User Reviews of the film [Shang-Chi and the Legend of the Ten Rings](https://www.imdb.com/title/tt9376612/reviews?ref_=tt_sa_3) from IMDB.\n",
        "\n",
        "(3) Collect all the abstracts research papers by using the query [smart health](https://citeseerx.ist.psu.edu/search?q=natural+language+processing&submit.x=0&submit.y=0&sort=rlv&t=doc) from CiteSeerX.\n",
        "\n",
        "(4) Collect the top 10,000 tweets by using hashtag [\"#blacklivesmatter\"](https://twitter.com/hashtag/blacklivesmatter) from Twitter with the following requirements: Location (Texas), Time frame (2021-01-01 to 2021-09-01). \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PuFPKhC0m1fd",
        "outputId": "9cb48e5e-072e-4b6e-b9c5-268414ced792"
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests, lxml, os, json\n",
        "from dateutil.parser import parse\n",
        "import pandas as pd\n",
        "\n",
        "proxies = {\n",
        "    'http': os.getenv('HTTP_PROXY')  # or just type proxy here without os.getenv()\n",
        "}\n",
        "\n",
        "headers = {\n",
        "    'User-agent':\n",
        "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.102 Safari/537.36 Edge/18.19582\"\n",
        "}\n",
        "\n",
        "df = pd.DataFrame()\n",
        "\n",
        "counter = 0\n",
        "\n",
        "for i in range(0, 112):\n",
        "    start = str(i * 10)\n",
        "\n",
        "    if i == 0:\n",
        "        url = 'https://citeseerx.ist.psu.edu/search?q=smart+health&submit.x=0&submit.y=0&sort=rlv&t=doc'\n",
        "    else:\n",
        "        url = 'https://citeseerx.ist.psu.edu/search?q=smart+health&t=doc&sort=rlv&start=' + start\n",
        "\n",
        "    html = requests.get(url, headers=headers, proxies=proxies).text\n",
        "    soup = BeautifulSoup(html, 'lxml')\n",
        "\n",
        "    for i in soup.findAll(\"div\", {'class': \"result\"}):\n",
        "        abstract = i.find(\"div\", {'class': \"pubabstract\"}).text\n",
        "\n",
        "        d = {\n",
        "            'abstract': abstract\n",
        "        }\n",
        "\n",
        "        df = df.append(d, ignore_index=True)\n",
        "\n",
        "\n",
        "df.to_csv(\"citeseer\")\n",
        "print(\"500 records download successful to Google Colab folders\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "500 records download successful to Google Colab folders\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfpMRCrRwN6Z"
      },
      "source": [
        "# **Question 2**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dCQEbDawWCw"
      },
      "source": [
        "(30 points) Here is a [legal case](https://github.com/unt-iialab/info5731-fall2021/blob/main/assignments/01-05-1%20%20Adams%20v%20Tanner.txt) we collected from westlaw, please follow the steps we mentioned in lesson 5 to clean the data:\n",
        "\n",
        "\n",
        "\n",
        "(1) Basic feature extraction using text data\n",
        "\n",
        "*   Number of sentences\n",
        "*   Number of words\n",
        "*   Number of characters\n",
        "*   Average word length\n",
        "*   Number of stopwords\n",
        "*   Number of special characters\n",
        "*   Number of numerics\n",
        "*   Number of uppercase words\n",
        "\n",
        "(2) Basic Text Pre-processing of text data\n",
        "\n",
        "*   Lower casing\n",
        "*   Punctuation removal\n",
        "*   Stopwords removal\n",
        "*   Frequent words removal\n",
        "*   Rare words removal\n",
        "*   Spelling correction\n",
        "*   Tokenization\n",
        "*   Stemming\n",
        "*   Lemmatization\n",
        "\n",
        "(3) Save all the **clean sentences** to a **csv file** (one column, each raw is a sentence) after finishing all the steps above.\n",
        "\n",
        "\n",
        "(4) Advance Text Processing\n",
        "\n",
        "*   Calculate the term frequency of all the terms.\n",
        "*   Print out top 10 1-gram, top 10 2-grams, and top 10 3-grams terms as features.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vATjQNTY8buA"
      },
      "source": [
        "from nltk import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "import numpy as np\n",
        "import nltk\n",
        "\n",
        "from urllib.request import urlopen\n",
        "import pandas as pd\n",
        "\n",
        "from textblob import TextBlob\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "nltk.download('punkt')\n",
        "\n",
        "data_url = \"https://raw.githubusercontent.com/unt-iialab/info5731-fall2021/main/assignments/01-05-1%20%20Adams%20v%20Tanner.txt\"\n",
        "legal_case_data = urlopen(data_url)\n",
        "\n",
        "data_input = pd.DataFrame()\n",
        "\n",
        "for line in legal_case_data:\n",
        "    extract = line.decode('utf-8').encode(\"ascii\", \"ignore\").rstrip()\n",
        "    d = {\n",
        "        'extract': extract\n",
        "    }\n",
        "    data_input = data_input.append(d, ignore_index=True)\n",
        "  \n",
        "data_input['avg_word'] = data_input['extract'].apply(lambda x: avg_word(x))\n",
        "data_input[['extract', 'avg_word']].head()\n",
        "\n",
        "\n",
        "data_input['word_count'] = data_input['extract'].apply(lambda x: len(str(x).split(\" \")))\n",
        "data_input[['extract', 'word_count']].head()\n",
        "\n",
        "\n",
        "data_input['char_count'] = data_input['extract'].str.len()\n",
        "data_input[['extract', 'char_count']].head()\n",
        "\n",
        "def avg_word(sentence):\n",
        "    if len(sentence) > 0:\n",
        "        words = sentence.split()\n",
        "        return (sum(len(word) for word in words)/len(words))\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stop = stopwords.words('english')\n",
        "\n",
        "data_input['stopwords'] = data_input['extract'].apply(lambda x: len([x for x in x.split() if x in stop]))\n",
        "data_input[['extract', 'stopwords']].head()\n",
        "\n",
        "data_input['hashtags'] = data_input['extract'].apply(lambda x: len([x for x in x.split() if x.startswith('#')]))\n",
        "data_input[['extract', 'hashtags']].head()\n",
        "\n",
        "data_input['numerics'] = data_input['extract'].apply(lambda x: len([x for x in x.split() if x.isdigit()]))\n",
        "data_input[['extract', 'numerics']].head()\n",
        "\n",
        "data_input['upper'] = data_input['extract'].apply(lambda x: len([x for x in x.split() if x.isupper()]))\n",
        "data_input[['extract', 'upper']].head()\n",
        "\n",
        "\n",
        "data_input['extract'] = data_input['extract'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
        "data_input['extract'].head()\n",
        "\n",
        "data_input['extract'] = data_input['extract'].str.replace('[^\\w\\s]', '')\n",
        "data_input['extract'].head()\n",
        "\n",
        "\n",
        "stop = stopwords.words('english')\n",
        "data_input['extract'] = data_input['extract'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
        "data_input['extract'].head()\n",
        "\n",
        "freq = pd.Series(' '.join(data_input['extract']).split()).value_counts()[:10]\n",
        "freq\n",
        "\n",
        "freq = list(freq.index)\n",
        "data_input['extract'] = data_input['extract'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n",
        "data_input['extract'].head()\n",
        "\n",
        "\n",
        "freq = pd.Series(' '.join(data_input['extract']).split()).value_counts()[-10:]\n",
        "freq\n",
        "\n",
        "freq = list(freq.index)\n",
        "data_input['extract'] = data_input['extract'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n",
        "data_input['extract'].head()\n",
        "\n",
        "data_input['extract'][:5].apply(lambda x: str(TextBlob(x).correct()))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "TextBlob(data_input['extract'][1]).words\n",
        "\n",
        "st = PorterStemmer()\n",
        "\n",
        "data_input['extract'][:5].apply(lambda x: \" \".join([st.stem(word) for word in x.split()]))\n",
        "\n",
        "\n",
        "nltk.download('wordnet')\n",
        "\n",
        "data_input['extract'] = data_input['extract'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
        "\n",
        "data_input['extract'].head()\n",
        "\n",
        "TextBlob(data_input['extract'][0]).ngrams(2)\n",
        "\n",
        "\n",
        "tf1 = (data_input['extract'][1:2]).apply(lambda x: pd.value_counts(x.split(\" \"))).sum(axis = 0).reset_index()\n",
        "tf1.columns = ['words','tf']\n",
        "tf1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "for i,word in enumerate(tf1['words']):\n",
        "  tf1.loc[i, 'idf'] = np.log(data_input.shape[0] / (len(data_input[data_input['extract'].str.contains(word)])))\n",
        "\n",
        "tf1\n",
        "\n",
        "\n",
        "tf1['tfidf'] = tf1['tf'] * tf1['idf']\n",
        "tf1\n",
        "\n",
        "\n",
        "myvocabulary = ['life', 'learning', 'game']\n",
        "corpus = {1: \"The game of life is a game of everlasting learning\", 2: \"The unexamined life is not worth living\", 3: \"Never stop learning\"}\n",
        "tfidf = TfidfVectorizer(vocabulary = myvocabulary, max_features=1000, lowercase=True, analyzer='word', stop_words='english', ngram_range=(1,3))\n",
        "tfs = tfidf.fit_transform(corpus.values())\n",
        "feature_names = tfidf.get_feature_names()\n",
        "corpus_index = [n for n in corpus]\n",
        "\n",
        "df = pd.DataFrame(tfs.T.todense(), index=feature_names, columns=corpus_index)\n",
        "print(df)\n",
        "\n",
        "bow = CountVectorizer(max_features=50, lowercase=True, ngram_range=(1,3),analyzer=\"word\")\n",
        "train_bow = bow.fit_transform(data_input['extract'])\n",
        "bow.get_feature_names()\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5mmYIfN8eYV"
      },
      "source": [
        "# **Question 3**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsi2y4z88ngX"
      },
      "source": [
        "(30 points). Write a python program to conduct **syntax and structure analysis** of the clean text you just saved above. The syntax and structure analysis includes: \n",
        "\n",
        "(1) Parts of Speech (POS) Tagging: Tag Parts of Speech of each word in the text, and calculate the total number of N(oun), V(erb), Adj(ective), Adv(erb), respectively.\n",
        "\n",
        "(2) Constituency Parsing and Dependency Parsing: print out the constituency parsing trees and dependency parsing trees of all the sentences. Using one sentence as an example to explain your understanding about the constituency parsing tree and dependency parsing tree.\n",
        "\n",
        "(3) Named Entity Recognition: Extract all the entities such as person names, organizations, locations, product names, and date from the clean texts, calculate the count of each entity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQKnPjPDHJHr"
      },
      "source": [
        "from urllib.request import urlopen\n",
        "import pandas as pd\n",
        "import spacy\n",
        "\n",
        "target_url = \"https://raw.githubusercontent.com/unt-iialab/info5731-fall2021/main/assignments/01-05-1%20%20Adams%20v%20Tanner.txt\"\n",
        "legal_case_data = urlopen(target_url)\n",
        "\n",
        "data = pd.DataFrame()\n",
        "\n",
        "for line in legal_case_data:\n",
        "  extract = line.decode('utf-8').encode(\"ascii\", \"ignore\").rstrip()\n",
        "  d = {\n",
        "    'extract': extract\n",
        "  }\n",
        "  data = data.append(d, ignore_index=True)\n",
        "\n",
        "data_str = str(legal_case_data)\n",
        "\n",
        "\n",
        "sp = spacy.load(\"en_core_web_sm\")\n",
        "extracts = sp(data_str)\n",
        "\n",
        "for word in extracts:\n",
        "  print(word.text + \"\\t \\t\" + str(word.pos))\n",
        "\n",
        "all_pos = extracts.count_by(spacy.attrs.POS)\n",
        "\n",
        "for k,v in all_pos.items():\n",
        "  print(extracts.vocab[k].text + \" \\t\" + str(v))\n",
        "\n",
        "for entity in extracts:\n",
        "  print(entity.text + \"\\t\\t\" + entity.label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWOtvT2rHNWy"
      },
      "source": [
        "**Write your explanations of the constituency parsing tree and dependency parsing tree here (Question 3-2):**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f652lyuJC6-o"
      },
      "source": [
        "# You explaination here\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTz-O8p2C6-o"
      },
      "source": [
        "# **Question 4**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcvacqLGC6-p"
      },
      "source": [
        "(20 points) Python Regular Expression.\n",
        "\n",
        "(1) Write a Python program to remove leading zeros from an IP address.\n",
        "\n",
        "ip = \"260.08.094.109\"\n",
        "\n",
        "\n",
        "(2) Write a Python Program to extract all the years from the following sentence.\n",
        "\n",
        "sentence = \"The 2010s were a dramatic decade, filled with ups and downs, more than 1000 stroies have happened. As the decade comes to a close, Insider took a look back at some of the biggest headline-grabbing stories, from 2010 to 2019. The result was 119 news stories that ranged from the heartwarming rescue of a Thai boys' soccer team from a flooded cave to the divisive election of President Donald Trump.\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Ewt9mBKC6-p",
        "outputId": "cd143bcf-95dd-45b3-9a51-f1c4c5f3c517"
      },
      "source": [
        "import re\n",
        "\n",
        "ip_address = input(\"enter ip address:\")\n",
        "address = ip_address.split(\".\") ## spliting using the split() functions\n",
        "address = [int(part) for part in address] ## converting every part to int\n",
        "address = [str(part) for part in address] ## converting each to str again before joining them\n",
        "ip_address = \".\".join(address) ## joining every part using the join() method\n",
        "print(\"ip address without leading zeros:\", ip_address)\n",
        "\n",
        "s = '''The 2010s were a dramatic decade, filled with ups and downs, more than 1000 stroies have happened.\n",
        "    As the decade comes to a close, Insider took a look back at some of the biggest headline-grabbing stories, from 2010 to 2019.\n",
        "    The result was 119 news stories that ranged from the heartwarming rescue of a Thai boys soccer team from a flooded\n",
        "    cave to the divisive election of President Donald Trump'''\n",
        "\n",
        "output = re.findall(r\"(2+\\d{3})\", s)\n",
        "print(\"All the years from the sentence:\", output)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "enter ip address:260.08.094.109\n",
            "ip address without leading zeros: 260.8.94.109\n",
            "All the years from the sentence: ['2010', '2010', '2019']\n"
          ]
        }
      ]
    }
  ]
}